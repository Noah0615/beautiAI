import os
import traceback
import torch
from PIL import Image
import cv2
import numpy as np
import joblib
import facer
from fb import get_db
from sklearn.cluster import KMeans
from flask import Flask, request, jsonify, render_template, send_from_directory, session
from werkzeug.utils import secure_filename
import base64
from io import BytesIO
import json
import ssl
import albumentations as A
from scipy import ndimage
from skimage import exposure, color

# ==============================================================================
# SSL ì¸ì¦ì„œ ì˜¤ë¥˜ í•´ê²° (facer ëª¨ë¸ ë‹¤ìš´ë¡œë“œìš©)
# ==============================================================================
ssl._create_default_https_context = ssl._create_unverified_context

# Flask ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸°í™”
app = Flask(__name__, template_folder='templates', static_folder='static')
app.secret_key = '12345'

# ì›¹ì•± ê¸°ë³¸ ì„¤ì •
UPLOAD_FOLDER = 'uploads'
if not os.path.exists(UPLOAD_FOLDER):
    os.makedirs(UPLOAD_FOLDER)

app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['ALLOWED_EXTENSIONS'] = {'png', 'jpg', 'jpeg', 'webp'}
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024

# AI ëª¨ë¸ ê´€ë ¨ ì „ì—­ ë³€ìˆ˜
MODEL_DIR = '.'
N_REPRESENTATIVE_COLORS = 7
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# í¼ìŠ¤ë„ ì»¬ëŸ¬ íƒ€ì…ë³„ ì •ë³´ ë°ì´í„°
CLUSTER_DESCRIPTIONS = {
    0: {"name": "Golden", "visual_name": "ê³¨ë“  íƒ€ì…", "description": "ëª…í™•í•œ ì›œ í†¤ì…ë‹ˆë‹¤.", "palette": ["#FFD700", "#FF7F50", "#FFA500", "#F4A460", "#FFFFE0"]},
    1: {"name": "Warm Beige", "visual_name": "ì›œ ë² ì´ì§€ íƒ€ì…", "description": "ì›œí•˜ì§€ë§Œ ì˜¬ë¦¬ë¸Œ ê¸°ìš´ì´ ìˆìŠµë‹ˆë‹¤.", "palette": ["#D2B48C", "#BC9A6A", "#8FBC8F", "#CD853F", "#DEB887"]},
    2: {"name": "Cool Rose", "visual_name": "ì¿¨ ë¡œì¦ˆ íƒ€ì…", "description": "í•‘í¬ì™€ ë ˆë“œê°€ í˜¼í•©ëœ ì¿¨ í†¤ì…ë‹ˆë‹¤.", "palette": ["#FFC0CB", "#FF69B4", "#DB7093", "#C71585", "#F08080"]},
    3: {"name": "Muted Clay", "visual_name": "ë®¤íŠ¸ í´ë ˆì´ íƒ€ì…", "description": "ì°¨ë¶„í•˜ê³  í†¤ ë‹¤ìš´ëœ ìƒ‰ê°ì…ë‹ˆë‹¤.", "palette": ["#BC9A6A", "#A0826D", "#8B7D6B", "#D2B48C", "#F5DEB3"]},
    4: {"name": "Warm Apricot", "visual_name": "ì›œ ì• í”„ë¦¬ì½§ íƒ€ì…", "description": "ì•ˆì •ì ì¸ ì˜¤ë Œì§€ ê³„ì—´ì˜ ì›œ í†¤ì…ë‹ˆë‹¤.", "palette": ["#FFCBA4", "#FF8C69", "#FFA07A", "#F4A460", "#FFDAB9"]},
    5: {"name": "Peachy Pink", "visual_name": "í”¼ì¹˜ í•‘í¬ íƒ€ì…", "description": "ì‚¬ë‘ìŠ¤ëŸ¬ìš´ Red-Pink ê³„ì—´ì…ë‹ˆë‹¤.", "palette": ["#FFCCCB", "#FFB6C1", "#FFA0B4", "#FF91A4", "#FFEFD5"]},
    6: {"name": "Honey Buff", "visual_name": "í—ˆë‹ˆ ë²„í”„ íƒ€ì…", "description": "ìœ ì‚¬ì„±ì´ ë§ì§€ë§Œ êµ¬ë¶„ ê°€ëŠ¥í•œ í†¤ì…ë‹ˆë‹¤.", "palette": ["#F0DC82", "#DAA520", "#CD853F", "#DEB887", "#F5DEB3"]},
    7: {"name": "Beige Rose", "visual_name": "ë² ì´ì§€ ë¡œì¦ˆ íƒ€ì…", "description": "ë¶€ë“œëŸ¬ìš´ ë² ì´ì§€ ë¡œì¦ˆ í†¤ì…ë‹ˆë‹¤.", "palette": ["#D2B48C", "#C4A484", "#BC9A6A", "#F5DEB3", "#E6D3C7"]}
}

# AI ëª¨ë¸ ê´€ë ¨ ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™”
kmeans_model = None
scaler = None
face_detector = None
face_parser = None
models_loaded = False

# ==============================================================================
# ê³ ê¸‰ ì¡°ëª… ë³´ì • í•¨ìˆ˜ë“¤
# ==============================================================================

def analyze_lighting_conditions(image_np):
    """ì´ë¯¸ì§€ì˜ ì¡°ëª… ìƒíƒœë¥¼ ë¶„ì„í•˜ì—¬ ë³´ì • ì „ëµì„ ê²°ì •"""
    # RGBë¥¼ LAB ìƒ‰ê³µê°„ìœ¼ë¡œ ë³€í™˜
    lab = cv2.cvtColor(image_np, cv2.COLOR_RGB2LAB)
    l_channel = lab[:, :, 0]
    
    # ì¡°ëª… ìƒíƒœ ë¶„ì„ ì§€í‘œë“¤
    mean_brightness = np.mean(l_channel)
    std_brightness = np.std(l_channel)
    
    # íˆìŠ¤í† ê·¸ë¨ ë¶„ì„
    hist, _ = np.histogram(l_channel, 256, [0, 256])
    
    # ì–´ë‘ìš´ í”½ì…€ê³¼ ë°ì€ í”½ì…€ì˜ ë¹„ìœ¨
    dark_pixels = np.sum(l_channel < 85) / l_channel.size
    bright_pixels = np.sum(l_channel > 170) / l_channel.size
    
    lighting_info = {
        'mean_brightness': mean_brightness,
        'std_brightness': std_brightness,
        'dark_ratio': dark_pixels,
        'bright_ratio': bright_pixels,
        'is_underexposed': mean_brightness < 120 and dark_pixels > 0.3,
        'is_overexposed': mean_brightness > 180 and bright_pixels > 0.2,
        'has_low_contrast': std_brightness < 25,
        'has_uneven_lighting': std_brightness > 50
    }
    
    return lighting_info

def white_balance_correction(image_np, method='gray_world'):
    """í™”ì´íŠ¸ ë°¸ëŸ°ìŠ¤ ë³´ì •"""
    image = image_np.astype(np.float32) / 255.0
    
    if method == 'gray_world':
        # Gray World ì•Œê³ ë¦¬ì¦˜: ì´ë¯¸ì§€ì˜ í‰ê·  ìƒ‰ìƒì´ íšŒìƒ‰ì´ ë˜ë„ë¡ ì¡°ì •
        mean_rgb = np.mean(image.reshape(-1, 3), axis=0)
        # íšŒìƒ‰ì (0.5, 0.5, 0.5)ì„ ê¸°ì¤€ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§
        scale_factors = 0.5 / (mean_rgb + 1e-8)
        corrected = image * scale_factors
        
    elif method == 'white_patch':
        # White Patch ì•Œê³ ë¦¬ì¦˜: ê°€ì¥ ë°ì€ ì ì´ í°ìƒ‰ì´ ë˜ë„ë¡ ì¡°ì •
        max_rgb = np.max(image.reshape(-1, 3), axis=0)
        scale_factors = 1.0 / (max_rgb + 1e-8)
        corrected = image * scale_factors
        
    elif method == 'illuminant_estimation':
        # ì¡°ëª… ì¶”ì • ê¸°ë°˜ ë³´ì • (ë‹¨ìˆœí™”ëœ ë²„ì „)
        # ì´ë¯¸ì§€ë¥¼ 3x3 ë¸”ë¡ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ê° ë¸”ë¡ì˜ í‰ê·  ê³„ì‚°
        h, w = image.shape[:2]
        block_means = []
        for i in range(0, h, h//3):
            for j in range(0, w, w//3):
                block = image[i:i+h//3, j:j+w//3]
                if block.size > 0:
                    block_means.append(np.mean(block.reshape(-1, 3), axis=0))
        
        if block_means:
            # ìµœëŒ€ ë°ê¸° ë¸”ë¡ì„ ê¸°ì¤€ìœ¼ë¡œ ì¡°ëª… ì¶”ì •
            block_means = np.array(block_means)
            max_idx = np.argmax(np.sum(block_means, axis=1))
            illuminant = block_means[max_idx]
            scale_factors = 0.9 / (illuminant + 1e-8)
            corrected = image * scale_factors
        else:
            corrected = image
    
    # ê°’ ë²”ìœ„ë¥¼ [0, 1]ë¡œ í´ë¦¬í•‘í•˜ê³  uint8ë¡œ ë³€í™˜
    corrected = np.clip(corrected, 0, 1)
    return (corrected * 255).astype(np.uint8)

def adaptive_histogram_equalization(image_np, clip_limit=3.0, tile_grid_size=(8, 8)):
    """ì ì‘ì  íˆìŠ¤í† ê·¸ë¨ í‰í™œí™” (CLAHE)"""
    # LAB ìƒ‰ê³µê°„ì—ì„œ L ì±„ë„ì—ë§Œ ì ìš©í•˜ì—¬ ìƒ‰ìƒ ì™œê³¡ ë°©ì§€
    lab = cv2.cvtColor(image_np, cv2.COLOR_RGB2LAB)
    l_channel = lab[:, :, 0]
    
    # CLAHE ì ìš©
    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)
    l_channel_eq = clahe.apply(l_channel)
    
    # LABë¥¼ ë‹¤ì‹œ ê²°í•©í•˜ê³  RGBë¡œ ë³€í™˜
    lab[:, :, 0] = l_channel_eq
    corrected = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)
    
    return corrected

def gamma_correction(image_np, gamma=1.0):
    """ê°ë§ˆ ë³´ì •"""
    if gamma == 1.0:
        return image_np
    
    # ê°ë§ˆ ë³´ì • ë£©ì—… í…Œì´ë¸” ìƒì„±
    inv_gamma = 1.0 / gamma
    table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype("uint8")
    
    # ë£©ì—… í…Œì´ë¸” ì ìš©
    return cv2.LUT(image_np, table)

def shadow_highlight_correction(image_np, shadow_amount=0.0, highlight_amount=0.0, shadow_width=50, highlight_width=50):
    """ê·¸ë¦¼ì/í•˜ì´ë¼ì´íŠ¸ ë³´ì •"""
    if shadow_amount == 0.0 and highlight_amount == 0.0:
        return image_np
    
    # RGBë¥¼ LABë¡œ ë³€í™˜
    lab = cv2.cvtColor(image_np, cv2.COLOR_RGB2LAB).astype(np.float32)
    l_channel = lab[:, :, 0] / 255.0
    
    # ê·¸ë¦¼ì ì˜ì—­ ë§ˆìŠ¤í¬ (ì–´ë‘ìš´ ë¶€ë¶„)
    shadow_mask = np.exp(-((l_channel - 0.0) ** 2) / (2 * (shadow_width / 255.0) ** 2))
    
    # í•˜ì´ë¼ì´íŠ¸ ì˜ì—­ ë§ˆìŠ¤í¬ (ë°ì€ ë¶€ë¶„)
    highlight_mask = np.exp(-((l_channel - 1.0) ** 2) / (2 * (highlight_width / 255.0) ** 2))
    
    # ì¡°ì • ì ìš©
    if shadow_amount != 0.0:
        l_channel = l_channel + shadow_amount * shadow_mask * (1.0 - l_channel)
    
    if highlight_amount != 0.0:
        l_channel = l_channel + highlight_amount * highlight_mask * (l_channel - 1.0)
    
    # ê°’ ë²”ìœ„ í´ë¦¬í•‘
    l_channel = np.clip(l_channel, 0, 1)
    lab[:, :, 0] = l_channel * 255.0
    
    # LABë¥¼ RGBë¡œ ë³€í™˜
    corrected = cv2.cvtColor(lab.astype(np.uint8), cv2.COLOR_LAB2RGB)
    
    return corrected

def unsharp_masking(image_np, strength=0.5, radius=1.0, threshold=0.0):
    """ì–¸ìƒ¤í”„ ë§ˆìŠ¤í‚¹ì„ í†µí•œ ì„ ëª…ë„ í–¥ìƒ"""
    if strength == 0.0:
        return image_np
    
    # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ë¥¼ ì´ìš©í•œ ì–¸ìƒ¤í”„ ë§ˆìŠ¤í¬ ìƒì„±
    blurred = cv2.GaussianBlur(image_np, (0, 0), radius)
    mask = image_np.astype(np.float32) - blurred.astype(np.float32)
    
    # ì„ê³„ê°’ ì ìš©
    if threshold > 0:
        mask = np.where(np.abs(mask) < threshold, 0, mask)
    
    # ì–¸ìƒ¤í”„ ë§ˆìŠ¤í¬ ì ìš©
    sharpened = image_np.astype(np.float32) + strength * mask
    sharpened = np.clip(sharpened, 0, 255).astype(np.uint8)
    
    return sharpened

def comprehensive_lighting_correction(image_np, lighting_info=None):
    """ì¢…í•©ì ì¸ ì¡°ëª… ë³´ì • íŒŒì´í”„ë¼ì¸"""
    if lighting_info is None:
        lighting_info = analyze_lighting_conditions(image_np)
    
    corrected = image_np.copy()
    correction_log = []
    
    # 1. í™”ì´íŠ¸ ë°¸ëŸ°ìŠ¤ ë³´ì • (í•­ìƒ ì ìš©)
    wb_method = 'gray_world'
    if lighting_info['bright_ratio'] > 0.15:
        wb_method = 'white_patch'
    elif lighting_info['has_uneven_lighting']:
        wb_method = 'illuminant_estimation'
    
    corrected = white_balance_correction(corrected, method=wb_method)
    correction_log.append(f"White balance: {wb_method}")
    
    # 2. ë…¸ì¶œ ë³´ì •
    if lighting_info['is_underexposed']:
        # ì–´ë‘ìš´ ì´ë¯¸ì§€: ê·¸ë¦¼ì ë³µêµ¬ + ê°ë§ˆ ë³´ì •
        corrected = shadow_highlight_correction(corrected, shadow_amount=0.3, highlight_amount=-0.1)
        gamma_value = 0.7  # ë°ê²Œ
        corrected = gamma_correction(corrected, gamma_value)
        correction_log.append(f"Underexposure correction: shadow lift + gamma {gamma_value}")
        
    elif lighting_info['is_overexposed']:
        # ë°ì€ ì´ë¯¸ì§€: í•˜ì´ë¼ì´íŠ¸ ë³µêµ¬
        corrected = shadow_highlight_correction(corrected, shadow_amount=0.0, highlight_amount=-0.4)
        gamma_value = 1.3  # ì–´ë‘¡ê²Œ
        corrected = gamma_correction(corrected, gamma_value)
        correction_log.append(f"Overexposure correction: highlight recovery + gamma {gamma_value}")
    
    # 3. ëŒ€ë¹„ ë³´ì •
    if lighting_info['has_low_contrast']:
        # ë‚®ì€ ëŒ€ë¹„: CLAHE ì ìš©
        clip_limit = 4.0 if lighting_info['std_brightness'] < 15 else 2.5
        corrected = adaptive_histogram_equalization(corrected, clip_limit=clip_limit)
        correction_log.append(f"Low contrast correction: CLAHE (clip_limit={clip_limit})")
    
    elif lighting_info['has_uneven_lighting']:
        # ë¶ˆê· ë“±í•œ ì¡°ëª…: ë¶€ë“œëŸ¬ìš´ CLAHE
        corrected = adaptive_histogram_equalization(corrected, clip_limit=2.0, tile_grid_size=(6, 6))
        correction_log.append("Uneven lighting correction: Soft CLAHE")
    
    # 4. ì„ ëª…ë„ í–¥ìƒ (ì„ íƒì )
    if lighting_info['std_brightness'] < 30:  # íë¦¿í•œ ì´ë¯¸ì§€
        corrected = unsharp_masking(corrected, strength=0.3, radius=1.2)
        correction_log.append("Sharpening applied")
    
    # 5. Albumentationsë¥¼ ì´ìš©í•œ ì¶”ê°€ ë³´ì • (ë¯¸ì„¸ ì¡°ì •)
    transform = A.Compose([
        A.RandomBrightnessContrast(
            brightness_limit=0.1, 
            contrast_limit=0.1, 
            p=0.5
        ),
        A.ColorJitter(
            brightness=0.05,
            contrast=0.05,
            saturation=0.05,
            hue=0.02,
            p=0.3
        )
    ])
    
    # 50% í™•ë¥ ë¡œ ë¯¸ì„¸ ì¡°ì • ì ìš©
    if np.random.random() < 0.5:
        augmented = transform(image=corrected)
        corrected = augmented['image']
        correction_log.append("Fine-tuning with Albumentations")
    
    return corrected, correction_log

def load_models():
    """AI ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
    global kmeans_model, scaler, face_detector, face_parser, models_loaded
    
    try:
        print("Loading AI models...")
        
        # ë¯¸ë¦¬ í›ˆë ¨ëœ K-means ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
        kmeans_model = joblib.load(os.path.join(MODEL_DIR, 'kmeans_model.joblib'))
        scaler = joblib.load(os.path.join(MODEL_DIR, 'scaler.joblib'))
        print("âœ“ K-means model and scaler loaded successfully.")

        # Facer ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì–¼êµ´ ê´€ë ¨ ëª¨ë¸ë“¤ ë¡œë“œ
        face_detector = facer.face_detector('retinaface/mobilenet', device=device)
        face_parser = facer.face_parser('farl/celebm/448', device=device)
        print("âœ“ Facer models loaded successfully.")
        print(f"âœ“ Using device: {device}")
        
        models_loaded = True
    except FileNotFoundError as e:
        print(f"âŒ ERROR: Model file not found. {e}")
    except Exception as e:
        print(f"âŒ An unexpected error occurred during model loading: {e}")
        traceback.print_exc()

def allowed_file(filename):
    """ì—…ë¡œë“œëœ íŒŒì¼ì´ í—ˆìš©ëœ í™•ì¥ìì¸ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜"""
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in app.config['ALLOWED_EXTENSIONS']

def extract_facial_part_colors(image: Image.Image, n_colors_per_part=7, apply_lighting_correction=True):
    """ì–¼êµ´ì—ì„œ í”¼ë¶€ ìƒ‰ìƒ íŠ¹ì§•(Lab ìƒ‰ê³µê°„)ì„ ì¶”ì¶œí•˜ëŠ” í•µì‹¬ í•¨ìˆ˜ (ì¡°ëª… ë³´ì • í¬í•¨)"""
    try:
        # ì´ë¯¸ì§€ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
        image_np = np.array(image)
        
        # ì¡°ëª… ë³´ì • ì ìš©
        correction_log = []
        if apply_lighting_correction:
            print("ğŸ”§ ì¡°ëª… ìƒíƒœ ë¶„ì„ ì¤‘...")
            lighting_info = analyze_lighting_conditions(image_np)
            
            print(f"   í‰ê·  ë°ê¸°: {lighting_info['mean_brightness']:.1f}")
            print(f"   ë°ê¸° í¸ì°¨: {lighting_info['std_brightness']:.1f}")
            print(f"   ì–´ë‘ìš´ í”½ì…€ ë¹„ìœ¨: {lighting_info['dark_ratio']:.2%}")
            print(f"   ë°ì€ í”½ì…€ ë¹„ìœ¨: {lighting_info['bright_ratio']:.2%}")
            
            # ì¢…í•©ì ì¸ ì¡°ëª… ë³´ì • ì ìš©
            print("ğŸ”§ ì¡°ëª… ë³´ì • ì ìš© ì¤‘...")
            image_np, correction_log = comprehensive_lighting_correction(image_np, lighting_info)
            
            for log in correction_log:
                print(f"   âœ“ {log}")
            
            # ë³´ì •ëœ ì´ë¯¸ì§€ë¥¼ PIL Imageë¡œ ë³€í™˜
            image = Image.fromarray(image_np)
        
        # ì´ë¯¸ì§€ë¥¼ ëª¨ë¸ ì…ë ¥ í¬ê¸°(448x448)ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        image_resized = image.resize((448, 448))
        image_resized_np = np.array(image_resized)
        
        # PyTorch í…ì„œë¡œ ë³€í™˜
        image_tensor = torch.from_numpy(image_resized_np).permute(2, 0, 1).unsqueeze(0).to(device)

        # ì–¼êµ´ ê°ì§€ ë° ë¶„í•  ìˆ˜í–‰
        with torch.inference_mode():
            faces = face_detector(image_tensor)
            
            if len(faces['scores']) == 0 or faces['scores'][0] < 0.5:
                return None, "ì–¼êµ´ì„ ê°ì§€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì •ë©´ì„ í–¥í•œ ì„ ëª…í•œ ì–¼êµ´ ì‚¬ì§„ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.", []
            
            faces = face_parser(image_tensor, faces)

        # ë¶„í•  ê²°ê³¼ì—ì„œ í”¼ë¶€ ì˜ì—­ë§Œ ì¶”ì¶œ
        seg_map = faces['seg']['logits'].argmax(dim=1).squeeze(0).cpu().numpy()
        
        # RGBë¥¼ Lab ìƒ‰ê³µê°„ìœ¼ë¡œ ë³€í™˜
        image_lab = cv2.cvtColor(image_resized_np, cv2.COLOR_RGB2Lab)

        # í”¼ë¶€ ì˜ì—­ ë§ˆìŠ¤í¬ ìƒì„±
        skin_mask = np.isin(seg_map, [1, 2])
        skin_pixels = image_lab[skin_mask]

        if len(skin_pixels) < n_colors_per_part:
            return None, "í”¼ë¶€ ì˜ì—­ì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì–¼êµ´ì´ ë” í¬ê²Œ ë‚˜ì˜¨ ì‚¬ì§„ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.", correction_log

        # K-means í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ëŒ€í‘œ ìƒ‰ìƒ ì¶”ì¶œ
        kmeans = KMeans(n_clusters=n_colors_per_part, n_init='auto', random_state=42)
        kmeans.fit(skin_pixels.astype(np.float32))
        
        return kmeans.cluster_centers_.astype(np.float32).flatten().reshape(1, -1), None, correction_log

    except Exception as e:
        traceback.print_exc()
        return None, f"ì´ë¯¸ì§€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", []

def get_cluster_info(cluster_id):
    """í´ëŸ¬ìŠ¤í„° IDì— í•´ë‹¹í•˜ëŠ” í¼ìŠ¤ë„ ì»¬ëŸ¬ ì •ë³´ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
    return CLUSTER_DESCRIPTIONS.get(cluster_id, CLUSTER_DESCRIPTIONS[0])

# ==============================================================================
# ì›¹ ë¼ìš°íŠ¸ ì •ì˜
# ==============================================================================

@app.route('/')
def index():
    """ë©”ì¸ í˜ì´ì§€ë¥¼ ë Œë”ë§í•˜ëŠ” ë¼ìš°íŠ¸"""
    return render_template('index.html')

@app.route('/analyze', methods=['POST'])
def analyze():
    """ì´ë¯¸ì§€ ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” ë©”ì¸ API ì—”ë“œí¬ì¸íŠ¸"""
    if not models_loaded:
        return jsonify({'error': 'AI ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 503

    try:
        image = None
        filename = f"upload_{np.datetime64('now').astype(int)}.jpg"
        
        # ì¡°ëª… ë³´ì • ì˜µì…˜ (ê¸°ë³¸ê°’: True)
        apply_correction = request.form.get('apply_lighting_correction', 'true').lower() == 'true'
        
        # ì´ë¯¸ì§€ ì…ë ¥ ì²˜ë¦¬
        if 'file' in request.files:
            file = request.files['file']
            if file.filename == '' or not allowed_file(file.filename):
                return jsonify({'error': 'ì˜ëª»ëœ íŒŒì¼ì…ë‹ˆë‹¤.'}), 400
            image = Image.open(file.stream).convert('RGB')
                
        elif request.json and 'image_data' in request.json:
            image_data = request.json['image_data'].split(',')[1]
            image = Image.open(BytesIO(base64.b64decode(image_data))).convert('RGB')
            # JSON ìš”ì²­ì—ì„œë„ ì¡°ëª… ë³´ì • ì˜µì…˜ í™•ì¸
            apply_correction = request.json.get('apply_lighting_correction', True)
        else:
            return jsonify({'error': 'ì´ë¯¸ì§€ íŒŒì¼ ë˜ëŠ” ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.'}), 400

        print(f"ğŸ” í¼ìŠ¤ë„ ì»¬ëŸ¬ ë¶„ì„ ì‹œì‘ (ì¡°ëª… ë³´ì •: {'ON' if apply_correction else 'OFF'})")
        
        # í¼ìŠ¤ë„ ì»¬ëŸ¬ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        lab_features, error_msg, correction_log = extract_facial_part_colors(
            image, 
            n_colors_per_part=N_REPRESENTATIVE_COLORS,
            apply_lighting_correction=apply_correction
        )
        
        if error_msg:
            return jsonify({'error': error_msg}), 400

        # íŠ¹ì§• ë°ì´í„° ì •ê·œí™” ë° ì˜ˆì¸¡
        scaled_features = scaler.transform(lab_features)
        predicted_cluster = kmeans_model.predict(scaled_features)[0]

        print(f"âœ… ë¶„ì„ ì™„ë£Œ: {get_cluster_info(predicted_cluster)['visual_name']}")

        # ê²°ê³¼ ë°ì´í„° ìƒì„±
        cluster_info = get_cluster_info(predicted_cluster)
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], secure_filename(filename))
        image.save(filepath, 'JPEG')

        result_data = {
            "success": True,
            "cluster_id": int(predicted_cluster),
            "personal_color_type": cluster_info["name"],
            "visual_name": cluster_info["visual_name"],
            "type_description": cluster_info["description"],
            "palette": cluster_info["palette"],
            "uploaded_image_url": f'/uploads/{filename}',
            "lighting_correction_applied": apply_correction,
            "correction_log": correction_log
        }
        return jsonify(result_data)

    except Exception as e:
        traceback.print_exc()
        return jsonify({'error': f'ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'}), 500

@app.route('/analyze_lighting', methods=['POST'])
def analyze_lighting():
    """ì¡°ëª… ë¶„ì„ ì „ìš© ì—”ë“œí¬ì¸íŠ¸"""
    try:
        image = None
        
        if 'file' in request.files:
            file = request.files['file']
            if file.filename == '' or not allowed_file(file.filename):
                return jsonify({'error': 'ì˜ëª»ëœ íŒŒì¼ì…ë‹ˆë‹¤.'}), 400
            image = Image.open(file.stream).convert('RGB')
                
        elif request.json and 'image_data' in request.json:
            image_data = request.json['image_data'].split(',')[1]
            image = Image.open(BytesIO(base64.b64decode(image_data))).convert('RGB')
        else:
            return jsonify({'error': 'ì´ë¯¸ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.'}), 400

        # ì¡°ëª… ìƒíƒœ ë¶„ì„
        image_np = np.array(image)
        lighting_info = analyze_lighting_conditions(image_np)
        
        # ë³´ì •ëœ ì´ë¯¸ì§€ ìƒì„±
        corrected_np, correction_log = comprehensive_lighting_correction(image_np, lighting_info)
        
        # ë³´ì •ëœ ì´ë¯¸ì§€ë¥¼ Base64ë¡œ ì¸ì½”ë”©
        corrected_image = Image.fromarray(corrected_np)
        buffer = BytesIO()
        corrected_image.save(buffer, format='JPEG')
        corrected_b64 = base64.b64encode(buffer.getvalue()).decode()
        
        return jsonify({
            'success': True,
            'lighting_analysis': lighting_info,
            'correction_log': correction_log,
            'corrected_image': f'data:image/jpeg;base64,{corrected_b64}'
        })

    except Exception as e:
        traceback.print_exc()
        return jsonify({'error': f'ì¡°ëª… ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'}), 500

@app.route('/uploads/<filename>')
def uploaded_file(filename):
    """ì—…ë¡œë“œëœ ì´ë¯¸ì§€ íŒŒì¼ì„ ì œê³µí•˜ëŠ” ë¼ìš°íŠ¸"""
    return send_from_directory(app.config['UPLOAD_FOLDER'], filename)
#store user's data in database
@app.route('/signup', methods=['POST'])
def signup():
    data = request.get_json(silent=True)
    if not data:
        return jsonify({'status': 'error', 'message': 'Invalid or missing JSON'}), 400

    name = data.get('name')
    password = data.get('password')
    email = data.get('email')
    sex = data.get('sex')

    if not name or not password or not email or not sex:
        return jsonify({'status': 'error', 'message': 'Missing required fields'}), 400

    db = get_db()
    users = db.collection('users')

    # Enforce unique email (recommended) â€” fast lookup by doc id
    # Use email as document id to guarantee uniqueness
    doc_ref = users.document(email)
    if doc_ref.get().exists:
        return jsonify({'status': 'error', 'message': 'Email already exists'}), 400

    # If you also want to enforce unique name, query:
    same_name = users.where('name', '==', name).limit(1).stream()
    if any(same_name):
        return jsonify({'status': 'error', 'message': 'Name already taken'}), 400

    # Create user document
    doc_ref.set({
        'name': name,
        'email': email,
        'password': password,  # âš ï¸ store hashed later
        'sex': sex,
        'image': None
    })

    return jsonify({'status': 'success'}), 200

#check username and passowrd when logging in
@app.route('/login', methods=['POST'])
def login():
    data = request.get_json(silent=True)
    name = data.get('name')
    password = data.get('password')

    if not name or not password:
        return jsonify({'status': 'error', 'message': 'Missing name or password'}), 400

    db = get_db()
    users = db.collection('users')

    # Keep compatibility with your existing frontend: login by name
    # (If multiple docs have same name, we take the first one found.)
    q = users.where('name', '==', name).limit(1).stream()
    user_doc = next(q, None)

    if not user_doc:
        return jsonify({'status': 'error', 'message': 'Invalid name or password'}), 401

    user = user_doc.to_dict()
    if user.get('password') != password:
        return jsonify({'status': 'error', 'message': 'Invalid name or password'}), 401

    session['user'] = {
        'name': user['name'],
        'email': user['email'],
        'sex': user['sex'],
        'image': None if not user.get('image') else f"/user_image/{user['name']}"
    }
    return jsonify({'status': 'success'}), 200

# logout route
@app.route('/logout', methods=['POST'])
def logout():
    session.clear() # ì„¸ì…˜ í´ë¦¬ì–´
    return jsonify({'status': 'success'}), 200

#fetch user info
@app.route('/me', methods=['GET'])
def get_profile():
    user = session.get('user')
    if not user:
        return jsonify({'status': 'error', 'message': 'Not logged in'}), 401
    return jsonify({'status': 'success', 'user': user}), 200

# ==============================================================================
# ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„
# ==============================================================================
if __name__ == '__main__':
    # ì„œë²„ ì‹œì‘ ì „ AI ëª¨ë¸ë“¤ì„ ë¯¸ë¦¬ ë¡œë“œ
    load_models()
    
    print("=" * 70)
    print(f"ğŸš€ Enhanced Personal Color Analysis Server Starting...")
    print(f"ğŸ“± Model Status: {'âœ… Loaded' if models_loaded else 'âŒ Failed'}")
    print(f"ğŸ–¥ï¸  Device: {device}")
    print(f"ğŸ”§ Advanced Lighting Corrections Available:")
    print(f"   â€¢ White Balance (Gray World, White Patch, Illuminant Estimation)")
    print(f"   â€¢ Adaptive Histogram Equalization (CLAHE)")
    print(f"   â€¢ Gamma Correction")
    print(f"   â€¢ Shadow/Highlight Recovery")
    print(f"   â€¢ Unsharp Masking")
    print(f"   â€¢ Albumentations Fine-tuning")
    print(f"ğŸŒ Server: http://127.0.0.1:5001")
    print("=" * 70)
    
    app.run(debug=True, host='0.0.0.0', port=5001)